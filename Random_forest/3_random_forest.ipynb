{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c566ce600c5b4f",
   "metadata": {},
   "source": [
    "# LA Wildfire Prediction: Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:38:32.021709Z",
     "start_time": "2025-04-26T20:38:32.011032Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For displaying plots in the notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76648cf7bc157cd0",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The following function loads the engineered wildfire dataset with all the features created in the previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b4148e996718260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:38:32.028578Z",
     "start_time": "2025-04-26T20:38:32.025755Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_engineered_data(file_path):\n",
    "   \n",
    "    print(f\"Loading engineered data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded data with shape: {df.shape}\")\n",
    "    \n",
    "    # Print the head of loaded data\n",
    "    print(\"\\nLoaded Engineered Data Head:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac4de90d6e66ee8",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Select relevant features for the model from the engineered dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edad4557b564b239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:38:32.044231Z",
     "start_time": "2025-04-26T20:38:32.039563Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_features(df):\n",
    "  \n",
    "    print(\"Selecting features...\")\n",
    "    \n",
    "    # Weather features\n",
    "    weather_features = ['AWND', 'PRCP', 'TMAX', 'TMIN']\n",
    "    \n",
    "    # Rolling features\n",
    "    rolling_features = []\n",
    "    for col in df.columns:\n",
    "        if any(suffix in col for suffix in ['_7D', '_3D', '_14D', '_prev']):\n",
    "            rolling_features.append(col)\n",
    "    \n",
    "    # Dryness features\n",
    "    dryness_features = []\n",
    "    for col in ['is_dry', 'dry_streak', 'days_since_rain', 'drought_index']:\n",
    "        if col in df.columns:\n",
    "            dryness_features.append(col)\n",
    "    \n",
    "    # Satellite features\n",
    "    satellite_features = []\n",
    "    if 'LST_Day_C' in df.columns:\n",
    "        satellite_features.append('LST_Day_C')\n",
    "    \n",
    "    # Time features\n",
    "    time_features = ['month', 'day_of_year', 'day_of_week', 'is_weekend']\n",
    "    if 'season_Spring' in df.columns:\n",
    "        time_features.extend(['season_Spring', 'season_Summer', 'season_Winter'])\n",
    "    \n",
    "    # Fire spread potential\n",
    "    special_features = []\n",
    "    if 'fire_spread_potential' in df.columns:\n",
    "        special_features.append('fire_spread_potential')\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = (weather_features + rolling_features + dryness_features +\n",
    "                   satellite_features + time_features + special_features)\n",
    "    \n",
    "    # Filter to include only features that exist in the dataframe\n",
    "    selected_features = [f for f in all_features if f in df.columns]\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features: {selected_features}\")\n",
    "    \n",
    "    # Create X (features) and y (target)\n",
    "    X = df[selected_features]\n",
    "    y = df['Fire_Occurred']  # Use Fire_Occurred as the target variable\n",
    "    \n",
    "    # Print the head of selected features\n",
    "    print(\"\\nSelected Features Head:\")\n",
    "    print(X.head())\n",
    "    \n",
    "    return X, y, selected_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ee4166266dab5",
   "metadata": {},
   "source": [
    "## Class Imbalance Handling\n",
    "Handle class imbalance in the dataset using class weights to give higher importance to the minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a42cad920a0bb43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:38:32.054283Z",
     "start_time": "2025-04-26T20:38:32.051207Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_class_imbalance(X, y, method='class_weight'):\n",
    "  \n",
    "    print(\"Handling class imbalance...\")\n",
    "    \n",
    "    # Check class distribution\n",
    "    class_counts = y.value_counts()\n",
    "    print(f\"Class distribution before balancing: {class_counts}\")\n",
    "    \n",
    "    # Calculate class imbalance ratio\n",
    "    if len(class_counts) > 1:\n",
    "        imbalance_ratio = class_counts.min() / class_counts.max()\n",
    "        print(f\"Class imbalance ratio: {imbalance_ratio:.4f}\")\n",
    "    \n",
    "    # For large datasets, using class weights is more efficient than resampling\n",
    "    if method == 'class_weight' or X.shape[0] > 100000:\n",
    "        print(\"Using class weights instead of resampling due to large dataset size\")\n",
    "        # Calculate class weights inversely proportional to class frequencies\n",
    "        class_weight = {0: 1.0,\n",
    "                       1: class_counts[0] / class_counts[1] if 1 in class_counts and class_counts[1] > 0 else 1.0}\n",
    "        print(f\"Class weights: {class_weight}\")\n",
    "        return X, y, class_weight\n",
    "    \n",
    "    # If no resampling is needed or possible\n",
    "    else:\n",
    "        print(\"No resampling applied\")\n",
    "        return X, y, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe27298420e82a16",
   "metadata": {},
   "source": [
    "## PCA Dimensionality Reduction\n",
    "Apply Principal Component Analysis (PCA) to reduce the dimensionality of the feature space while retaining most of the variance. This helps improve model performance and reduce overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7d59cc330dc21df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:38:32.063222Z",
     "start_time": "2025-04-26T20:38:32.060849Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_pca(X, explained_variance=0.95):\n",
    "   \n",
    "    print(f\"\\nApplying PCA to retain {explained_variance*100:.1f}% variance...\")\n",
    "    \n",
    "    # 1. Create PCA model\n",
    "    pca = PCA(n_components=explained_variance, random_state=42)\n",
    "    \n",
    "    # 2. Fit and transform the data\n",
    "    X_pca_array = pca.fit_transform(X)\n",
    "    \n",
    "    # 3. Create principal component column names\n",
    "    component_names = [f'PC{i+1}' for i in range(X_pca_array.shape[1])]\n",
    "    \n",
    "    # 4. Convert to DataFrame\n",
    "    X_pca = pd.DataFrame(X_pca_array, columns=component_names)\n",
    "    \n",
    "    print(f\"PCA reduced features from {X.shape[1]} to {X_pca.shape[1]} components.\")\n",
    "    \n",
    "    return X_pca, pca, component_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d99c539b9a5632",
   "metadata": {},
   "source": [
    "## Random Forest Model Training\n",
    "Train a Random Forest classifier with optimized hyperparameters using RandomizedSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62e4781cf3cd5354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:38:32.072527Z",
     "start_time": "2025-04-26T20:38:32.069147Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forest(X, y, class_weight=None):\n",
    "   \n",
    "    print(\"Training Random Forest model...\")\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Create a pipeline with preprocessing and model\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Define hyperparameters for random search\n",
    "    param_dist = {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [10, 20, 30, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4],\n",
    "        'classifier__class_weight': [class_weight, 'balanced', 'balanced_subsample']\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Perform random search with cross-validation\n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        cv=3,\n",
    "        scoring='f1',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters\n",
    "    best_params = random_search.best_params_\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(best_model, X_test, y_test)\n",
    "    \n",
    "    return best_model, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af739b44a643a9",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate the trained model using various metrics and visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d365fea057d1198d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:38:32.084195Z",
     "start_time": "2025-04-26T20:38:32.079851Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "  \n",
    "    print(\"Evaluating model...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['No Fire', 'Fire'], \n",
    "                yticklabels=['No Fire', 'Fire'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs('../reports/figures', exist_ok=True)\n",
    "    plt.savefig('../reports/figures/confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance\n",
    "    if hasattr(model, 'named_steps') and hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
    "        feature_importances = pd.DataFrame(\n",
    "            model.named_steps['classifier'].feature_importances_,\n",
    "            index=X_test.columns,\n",
    "            columns=['Importance']\n",
    "        ).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_importances.head(10))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        feature_importances.head(15).plot(kind='barh')\n",
    "        plt.title('Feature Importance')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../reports/figures/feature_importance.png')\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7679f5f6b9319a2",
   "metadata": {},
   "source": [
    "## Save Model and Features\n",
    "Save the trained model, PCA model, and feature names for later use in evaluation and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "511f37847ffd513f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:38:32.093869Z",
     "start_time": "2025-04-26T20:38:32.090854Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, feature_names, output_path, pca_model=None, component_names=None):\n",
    "    \n",
    "    print(f\"Saving model to {output_path}...\")\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(model, output_path)\n",
    "    \n",
    "    # Save PCA model if provided\n",
    "    if pca_model is not None:\n",
    "        pca_path = os.path.join(os.path.dirname(output_path), 'pca_model.pkl')\n",
    "        joblib.dump(pca_model, pca_path)\n",
    "        print(f\"PCA model saved to {pca_path}\")\n",
    "    \n",
    "    # Save feature names - use component names if PCA was applied\n",
    "    feature_path = os.path.join(os.path.dirname(output_path), 'feature_names.txt')\n",
    "    with open(feature_path, 'w') as f:\n",
    "        if component_names is not None:\n",
    "            for feature in component_names:\n",
    "                f.write(f\"{feature}\\n\")\n",
    "        else:\n",
    "            for feature in feature_names:\n",
    "                f.write(f\"{feature}\\n\")\n",
    "    \n",
    "    print(f\"Model saved to {output_path}\")\n",
    "    print(f\"Feature names saved to {feature_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dfec60546cf677",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Run the complete Random Forest model training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb1a4e61712e477f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:38:32.102907Z",
     "start_time": "2025-04-26T20:38:32.099957Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function for training the Random Forest model.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Define file paths\n",
    "    input_path = \"../data/processed/engineered_la_fire_data.csv\"\n",
    "    output_path = \"../models/la_fire_random_forest_model.pkl\"\n",
    "    \n",
    "    # Load engineered data\n",
    "    df = load_engineered_data(input_path)\n",
    "    \n",
    "    # Select features\n",
    "    X, y, selected_features = select_features(df)\n",
    "    \n",
    "    # Handle class imbalance\n",
    "    X_balanced, y_balanced, class_weight = handle_class_imbalance(X, y, method='class_weight')\n",
    "    \n",
    "    # Apply PCA with original explained variance\n",
    "    X_pca, pca_model, component_names = apply_pca(X_balanced, explained_variance=0.95)\n",
    "\n",
    "    \n",
    "    # Train model\n",
    "    model, X_test, y_test = train_random_forest(X_pca, y_balanced, class_weight)\n",
    "    \n",
    "    # Save model with PCA components\n",
    "    save_model(model, selected_features, output_path, pca_model, component_names)\n",
    "    \n",
    "    # Save PCA model separately for use in evaluation\n",
    "    pca_path = os.path.join(os.path.dirname(output_path), 'pca_model.pkl')\n",
    "    joblib.dump(pca_model, pca_path)\n",
    "    \n",
    "    # Print execution time\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution completed in {execution_time:.2f} seconds\")\n",
    "    print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    print(\"Random Forest model training completed successfully!\")\n",
    "    \n",
    "    return model, X_test, y_test, selected_features, component_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a0886c9e04a0798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:39:26.867277Z",
     "start_time": "2025-04-26T20:38:32.109083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at: 2025-04-26 14:22:12\n",
      "Loading engineered data from ../data/processed/engineered_la_fire_data.csv...\n",
      "Loaded data with shape: (224986, 49)\n",
      "\n",
      "Loaded Engineered Data Head:\n",
      "         date  Fire_Occurred STATION NAME  AWND  DAPR  MDPR  PGTM  PRCP  TAVG  \\\n",
      "0  2014-12-27              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "1  2014-12-28              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2  2014-12-29              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "3  2014-12-30              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4  2014-12-31              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "   ...  season_Summer  season_Fall  TMAX_3D  TMIN_3D  PRCP_3D  PRCP_14D  \\\n",
      "0  ...          False         True      0.0      0.0      0.0       0.0   \n",
      "1  ...          False         True      0.0      0.0      0.0       0.0   \n",
      "2  ...          False         True      0.0      0.0      0.0       0.0   \n",
      "3  ...          False         True      0.0      0.0      0.0       0.0   \n",
      "4  ...          False         True      0.0      0.0      0.0       0.0   \n",
      "\n",
      "   AWND_3D  days_since_rain  drought_index  fire_spread_potential  \n",
      "0      0.0              1.0            0.0                   0.01  \n",
      "1      0.0              2.0            0.0                   0.02  \n",
      "2      0.0              3.0            0.0                   0.03  \n",
      "3      0.0              4.0            0.0                   0.04  \n",
      "4      0.0              5.0            0.0                   0.05  \n",
      "\n",
      "[5 rows x 49 columns]\n",
      "Selecting features...\n",
      "Selected 25 features: ['AWND', 'PRCP', 'TMAX', 'TMIN', 'PRCP_7D', 'AWND_7D', 'PRCP_prev', 'AWND_prev', 'TMAX_3D', 'TMIN_3D', 'PRCP_3D', 'PRCP_14D', 'AWND_3D', 'is_dry', 'dry_streak', 'days_since_rain', 'drought_index', 'LST_Day_C', 'month', 'day_of_year', 'day_of_week', 'is_weekend', 'season_Spring', 'season_Summer', 'fire_spread_potential']\n",
      "\n",
      "Selected Features Head:\n",
      "   AWND  PRCP  TMAX  TMIN  PRCP_7D  AWND_7D  PRCP_prev  AWND_prev  TMAX_3D  \\\n",
      "0   0.0   0.0   0.0   0.0      0.0      0.0        0.0        0.0      0.0   \n",
      "1   0.0   0.0   0.0   0.0      0.0      0.0        0.0        0.0      0.0   \n",
      "2   0.0   0.0   0.0   0.0      0.0      0.0        0.0        0.0      0.0   \n",
      "3   0.0   0.0   0.0   0.0      0.0      0.0        0.0        0.0      0.0   \n",
      "4   0.0   0.0   0.0   0.0      0.0      0.0        0.0        0.0      0.0   \n",
      "\n",
      "   TMIN_3D  ...  days_since_rain  drought_index  LST_Day_C  month  \\\n",
      "0      0.0  ...              1.0            0.0  12.674622     12   \n",
      "1      0.0  ...              2.0            0.0  12.674622     12   \n",
      "2      0.0  ...              3.0            0.0  12.674622     12   \n",
      "3      0.0  ...              4.0            0.0  12.674622     12   \n",
      "4      0.0  ...              5.0            0.0  12.674622     12   \n",
      "\n",
      "   day_of_year  day_of_week  is_weekend  season_Spring  season_Summer  \\\n",
      "0          361            5           1          False          False   \n",
      "1          362            6           1          False          False   \n",
      "2          363            0           0          False          False   \n",
      "3          364            1           0          False          False   \n",
      "4          365            2           0          False          False   \n",
      "\n",
      "   fire_spread_potential  \n",
      "0                   0.01  \n",
      "1                   0.02  \n",
      "2                   0.03  \n",
      "3                   0.04  \n",
      "4                   0.05  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Handling class imbalance...\n",
      "Class distribution before balancing: Fire_Occurred\n",
      "0    210508\n",
      "1     14478\n",
      "Name: count, dtype: int64\n",
      "Class imbalance ratio: 0.0688\n",
      "Using class weights instead of resampling due to large dataset size\n",
      "Class weights: {0: 1.0, 1: 14.539853570935213}\n",
      "\n",
      "Applying PCA to retain 95.0% variance...\n",
      "PCA reduced features from 25 to 3 components.\n",
      "Training Random Forest model...\n",
      "Training set shape: (179988, 3)\n",
      "Testing set shape: (44998, 3)\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters: {'classifier__n_estimators': 100, 'classifier__min_samples_split': 10, 'classifier__min_samples_leaf': 1, 'classifier__max_depth': None, 'classifier__class_weight': 'balanced'}\n",
      "Evaluating model...\n",
      "Accuracy: 0.9694\n",
      "F1 Score: 0.7342\n",
      "ROC AUC Score: 0.9716\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     42102\n",
      "           1       0.83      0.66      0.73      2896\n",
      "\n",
      "    accuracy                           0.97     44998\n",
      "   macro avg       0.90      0.82      0.86     44998\n",
      "weighted avg       0.97      0.97      0.97     44998\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "     Importance\n",
      "PC1    0.464723\n",
      "PC2    0.282216\n",
      "PC3    0.253061\n",
      "Saving model to ../models/la_fire_random_forest_model.pkl...\n",
      "PCA model saved to ../models/pca_model.pkl\n",
      "Model saved to ../models/la_fire_random_forest_model.pkl\n",
      "Feature names saved to ../models/feature_names.txt\n",
      "Execution completed in 53.08 seconds\n",
      "Finished at: 2025-04-26 14:23:05\n",
      "Random Forest model training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Execute the Random Forest model training pipeline\n",
    "model, X_test, y_test, selected_features, component_names = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d613a71-8bc1-4e30-a67e-4fa9b3d65c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
