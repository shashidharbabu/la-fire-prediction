{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6688aeb92c4e75b",
   "metadata": {},
   "source": [
    "# LA Wildfire Prediction: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f97665b930f4e3b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:13:02.127703Z",
     "start_time": "2025-04-26T20:13:02.119309Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# For displaying plots in the notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c52973a145fc5",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The following function loads the raw wildfire dataset from CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fd55ab59424a809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:13:02.143754Z",
     "start_time": "2025-04-26T20:13:02.140349Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded data with shape: {df.shape}\")\n",
    "    \n",
    "    # Print the head of raw data\n",
    "    print(\"\\nRaw Data Head:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7ba868fd79ae",
   "metadata": {},
   "source": [
    "## Date Conversion\n",
    "Convert date columns to datetime format for time-based analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6912a030be9b01f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:13:02.152380Z",
     "start_time": "2025-04-26T20:13:02.149810Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_date_columns(df):\n",
    "    print(\"Converting date columns...\")\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151910f7edd79429",
   "metadata": {},
   "source": [
    "## Missing Value Handling\n",
    "Handle missing values in the dataset by replacing them with 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa88da5e8123de14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:13:02.159511Z",
     "start_time": "2025-04-26T20:13:02.157273Z"
    }
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    print(\"Handling missing values...\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"Missing values before replacement:\\n{missing_values[missing_values > 0]}\")\n",
    "    \n",
    "    # Replace all missing values with 0\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # Check missing values after replacement\n",
    "    missing_values_after = df.isnull().sum()\n",
    "    print(f\"Missing values after replacement:\\n{missing_values_after[missing_values_after > 0]}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d483b45589a0b8",
   "metadata": {},
   "source": [
    "## Data Integrity Checks\n",
    "Perform data integrity checks to ensure data quality:\n",
    "- Remove duplicates\n",
    "- Fix negative precipitation values\n",
    "- Remove records with unreasonable temperature values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c062dc369c09d354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:13:02.175603Z",
     "start_time": "2025-04-26T20:13:02.171267Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_data_integrity(df):\n",
    " \n",
    "    print(\"Checking data integrity...\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Found {duplicates} duplicate rows\")\n",
    "    if duplicates > 0:\n",
    "        df = df.drop_duplicates()\n",
    "        print(f\"Removed {duplicates} duplicate rows\")\n",
    "    \n",
    "    # Check for invalid values in key columns\n",
    "    print(\"Checking for invalid values in key columns...\")\n",
    "    \n",
    "    # Ensure precipitation is non-negative\n",
    "    if 'PRCP' in df.columns:\n",
    "        invalid_prcp = (df['PRCP'] < 0).sum()\n",
    "        if invalid_prcp > 0:\n",
    "            print(f\"Found {invalid_prcp} rows with negative precipitation\")\n",
    "            df.loc[df['PRCP'] < 0, 'PRCP'] = 0\n",
    "            print(\"Fixed negative precipitation values\")\n",
    "    \n",
    "    # Ensure temperature values are within reasonable range for LA\n",
    "    if 'TMAX' in df.columns and 'TMIN' in df.columns:\n",
    "        invalid_tmax = ((df['TMAX'] < -10) | (df['TMAX'] > 120)).sum()\n",
    "        invalid_tmin = ((df['TMIN'] < -10) | (df['TMIN'] > 100)).sum()\n",
    "        \n",
    "        if invalid_tmax > 0:\n",
    "            print(f\"Found {invalid_tmax} rows with unreasonable TMAX values\")\n",
    "            df = df[(df['TMAX'] >= -10) & (df['TMAX'] <= 120)]\n",
    "        \n",
    "        if invalid_tmin > 0:\n",
    "            print(f\"Found {invalid_tmin} rows with unreasonable TMIN values\")\n",
    "            df = df[(df['TMIN'] >= -10) & (df['TMIN'] <= 100)]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e94c919ae84119",
   "metadata": {},
   "source": [
    "## Column Dropping\n",
    "Drop specified columns from the dataframe. In this case, we only drop 'fire_count' and keep 'Fire_Occurred'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3993d9ffaf484e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:13:02.182722Z",
     "start_time": "2025-04-26T20:13:02.180503Z"
    }
   },
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "\n",
    "    print(\"Dropping specified columns...\")\n",
    "    \n",
    "    # Check if the columns exist before dropping\n",
    "    columns_to_drop = ['fire_count']  # Only drop fire_count, keep Fire_Occurred\n",
    "    existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "    \n",
    "    if existing_columns:\n",
    "        df = df.drop(columns=existing_columns)\n",
    "        print(f\"Dropped columns: {existing_columns}\")\n",
    "    else:\n",
    "        print(\"Specified columns not found in the dataframe\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6466e358055675",
   "metadata": {},
   "source": [
    "## Save Processed Data\n",
    "Save the processed dataframe to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d418e6850bb41de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:13:02.189130Z",
     "start_time": "2025-04-26T20:13:02.186935Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_processed_data(df, output_path):\n",
    "   \n",
    "    print(f\"Saving processed data to {output_path}...\")\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved processed data with shape: {df.shape}\")\n",
    "    \n",
    "    # Print the head of processed data\n",
    "    print(\"\\nProcessed Data Head:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8998296386a1ef7",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Run the complete preprocessing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8c3b1b04cf34c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:13:02.195101Z",
     "start_time": "2025-04-26T20:13:02.193039Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define file paths\n",
    "    input_path = \"../data/raw/FINAL_LA_FIRE_ML_DATA.csv\"\n",
    "    output_path = \"../data/processed/processed_la_fire_data.csv\"\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data(input_path)\n",
    "    \n",
    "    # Preprocess data\n",
    "    df = convert_date_columns(df)\n",
    "    df = handle_missing_values(df)\n",
    "    df = check_data_integrity(df)\n",
    "    \n",
    "    # Drop specified columns\n",
    "    df = drop_columns(df)\n",
    "    \n",
    "    # Save processed data\n",
    "    save_processed_data(df, output_path)\n",
    "    \n",
    "    print(\"Preprocessing completed successfully!\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fbec463e26d302a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:13:04.375804Z",
     "start_time": "2025-04-26T20:13:02.203897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/raw/FINAL_LA_FIRE_ML_DATA.csv...\n",
      "Loaded data with shape: (225016, 36)\n",
      "\n",
      "Raw Data Head:\n",
      "         date  fire_count  Fire_Occurred STATION NAME  AWND  DAPR  MDPR  PGTM  \\\n",
      "0  2014-12-27           0              0     NaN  NaN   NaN   NaN   NaN   NaN   \n",
      "1  2014-12-28           0              0     NaN  NaN   NaN   NaN   NaN   NaN   \n",
      "2  2014-12-29           0              0     NaN  NaN   NaN   NaN   NaN   NaN   \n",
      "3  2014-12-30           0              0     NaN  NaN   NaN   NaN   NaN   NaN   \n",
      "4  2014-12-31           0              0     NaN  NaN   NaN   NaN   NaN   NaN   \n",
      "\n",
      "   PRCP  ...  WT11  year  month  PRCP_7D  AWND_7D  PRCP_prev  AWND_prev  \\\n",
      "0   NaN  ...   NaN   NaN    NaN      NaN      NaN        NaN        NaN   \n",
      "1   NaN  ...   NaN   NaN    NaN      NaN      NaN        NaN        NaN   \n",
      "2   NaN  ...   NaN   NaN    NaN      NaN      NaN        NaN        NaN   \n",
      "3   NaN  ...   NaN   NaN    NaN      NaN      NaN        NaN        NaN   \n",
      "4   NaN  ...   NaN   NaN    NaN      NaN      NaN        NaN        NaN   \n",
      "\n",
      "   is_dry  dry_streak  LST_Day_C  \n",
      "0     NaN         NaN  12.674622  \n",
      "1     NaN         NaN  12.674622  \n",
      "2     NaN         NaN  12.674622  \n",
      "3     NaN         NaN  12.674622  \n",
      "4     NaN         NaN  12.674622  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "Converting date columns...\n",
      "Handling missing values...\n",
      "Missing values before replacement:\n",
      "STATION            5\n",
      "NAME               5\n",
      "AWND               5\n",
      "DAPR          224456\n",
      "MDPR          224460\n",
      "PGTM          223500\n",
      "PRCP               5\n",
      "TAVG          178212\n",
      "TMAX          121077\n",
      "TMIN          121263\n",
      "TOBS          191517\n",
      "WDF2          196188\n",
      "WESD          221738\n",
      "WESF          221704\n",
      "WSF2          196187\n",
      "WT01          216245\n",
      "WT02          224112\n",
      "WT03          224643\n",
      "WT05          225010\n",
      "WT06          225010\n",
      "WT07          224997\n",
      "WT08          215050\n",
      "WT10          225015\n",
      "WT11          225013\n",
      "year               5\n",
      "month              5\n",
      "PRCP_7D            5\n",
      "AWND_7D            5\n",
      "PRCP_prev          6\n",
      "AWND_prev          6\n",
      "is_dry             5\n",
      "dry_streak         5\n",
      "dtype: int64\n",
      "Missing values after replacement:\n",
      "Series([], dtype: int64)\n",
      "Checking data integrity...\n",
      "Found 0 duplicate rows\n",
      "Checking for invalid values in key columns...\n",
      "Found 26 rows with unreasonable TMAX values\n",
      "Found 19 rows with unreasonable TMIN values\n",
      "Dropping specified columns...\n",
      "Dropped columns: ['fire_count']\n",
      "Saving processed data to ../data/processed/processed_la_fire_data.csv...\n",
      "Saved processed data with shape: (224986, 35)\n",
      "\n",
      "Processed Data Head:\n",
      "        date  Fire_Occurred STATION NAME  AWND  DAPR  MDPR  PGTM  PRCP  TAVG  \\\n",
      "0 2014-12-27              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "1 2014-12-28              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2 2014-12-29              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "3 2014-12-30              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4 2014-12-31              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "   ...  WT11  year  month  PRCP_7D  AWND_7D  PRCP_prev  AWND_prev  is_dry  \\\n",
      "0  ...   0.0   0.0    0.0      0.0      0.0        0.0        0.0     0.0   \n",
      "1  ...   0.0   0.0    0.0      0.0      0.0        0.0        0.0     0.0   \n",
      "2  ...   0.0   0.0    0.0      0.0      0.0        0.0        0.0     0.0   \n",
      "3  ...   0.0   0.0    0.0      0.0      0.0        0.0        0.0     0.0   \n",
      "4  ...   0.0   0.0    0.0      0.0      0.0        0.0        0.0     0.0   \n",
      "\n",
      "   dry_streak  LST_Day_C  \n",
      "0         0.0  12.674622  \n",
      "1         0.0  12.674622  \n",
      "2         0.0  12.674622  \n",
      "3         0.0  12.674622  \n",
      "4         0.0  12.674622  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "Preprocessing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Execute the preprocessing pipeline\n",
    "preprocessed_df = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6843f89-7bed-498b-b298-a66d2a6e627a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
