{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82847d62-32b8-474d-a081-71575d8c86f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b8dd12-3050-465a-b5cc-6d2d7ece174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/vineethrayadurgam/Desktop/Machine Learning 245/FINAL_LA_FIRE_ML_DATA_MERGED.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab83f21-5ae2-43cb-baaf-2c2b0b409ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40f1abd9-3827-4398-b12e-40f12af62335",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['DAPR', 'MDPR', 'PGTM'] + [f'WT{str(i).zfill(2)}' for i in range(1, 12)]\n",
    "cols_to_drop += ['TOBS', 'WDF2', 'WESD', 'WESF', 'WSF2']\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
    "df = df.drop(columns=existing_cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d86bccd6-c887-4ce0-a8a5-44a3cb6dad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['NAME']) #Same as station column representing the name of the station, redundant column\n",
    "\n",
    "# Encode STATION using category codes\n",
    "df['STATION'] = df['STATION'].astype(str)\n",
    "df['STATION_ENC'] = df['STATION'].astype('category').cat.codes\n",
    "df = df.drop(columns=['STATION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fffcf3a-8ded-44fd-8368-bd0edeb9a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Start with a fresh copy of the raw dataset\n",
    "df_winsor = df.copy()\n",
    "df_transform = df_winsor.copy()  # We'll work off this for transformations\n",
    "\n",
    "### Winsorize heavy-tailed columns\n",
    "winsor_cols = ['PRCP', 'PRCP_prev', 'PRCP_7D', 'AWND', 'AWND_prev', 'AWND_7D', 'fire_count']\n",
    "for col in winsor_cols:\n",
    "    df_transform[col + '_win'] = winsorize(df_transform[col], limits=[0.01, 0.01])\n",
    "\n",
    "### Two-step transformation for precipitation variables\n",
    "precip_cols = ['PRCP', 'PRCP_prev', 'PRCP_7D']\n",
    "epsilon = 1e-8\n",
    "for col in precip_cols:\n",
    "    win_col = col + '_win'\n",
    "    df_transform[col + '_nonzero'] = (df_transform[win_col] > 0).astype(int)\n",
    "    df_transform[col + '_log'] = np.where(\n",
    "        df_transform[win_col] > 0,\n",
    "        np.log(df_transform[win_col] + epsilon),\n",
    "        0\n",
    "    )\n",
    "\n",
    "### Clip temperature values (physical plausibility bounds)\n",
    "temp_cols = ['TMAX', 'TMIN', 'TAVG']\n",
    "for col in temp_cols:\n",
    "    df_transform[col + '_clipped'] = np.clip(df_transform[col], -30, 130)\n",
    "\n",
    "### Standardize the clipped temperatures\n",
    "scaler = StandardScaler()\n",
    "df_transform[[col + '_scaled' for col in temp_cols]] = scaler.fit_transform(\n",
    "    df_transform[[col + '_clipped' for col in temp_cols]]\n",
    ")\n",
    "\n",
    "### Drop the raw original columns\n",
    "cols_to_drop = winsor_cols + temp_cols\n",
    "df_transform.drop(columns=cols_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d889a610-04d8-445a-a18e-f68c077e65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform = df_transform.drop(columns= [\"fire_count_win\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0cd752b-be94-4e1f-8aa0-a67dbb5c586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance per PC:\n",
      "PC1: 0.2739\n",
      "PC2: 0.1911\n",
      "PC3: 0.1159\n",
      "PC4: 0.1006\n",
      "PC5: 0.0921\n",
      "PC6: 0.0570\n",
      "PC7: 0.0396\n",
      "PC8: 0.0269\n",
      "PC9: 0.0258\n",
      "PC10: 0.0216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Drop target\n",
    "X_pca_input = df_transform.drop(columns=['Fire_Occurred'], errors='ignore').select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Step 2: Impute + Scale\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_imputed = imputer.fit_transform(X_pca_input)\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# Step 3: Fit PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 4: Wrap into a DataFrame\n",
    "pca_cols = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
    "df_pca = pd.DataFrame(X_pca, columns=pca_cols)\n",
    "\n",
    "# Optional: Check variance explained\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance per PC:\")\n",
    "for i, v in enumerate(explained_variance[:10]):\n",
    "    print(f\"PC{i+1}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0030287c-c83a-453e-948c-fac217d61db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform['fire_risk_index'] = (\n",
    "    (df_transform['TMAX_scaled'] ** 1.5 + df_transform['AWND_win']) /\n",
    "    (df_transform['PRCP_7D_win'] + 0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9326447-7bb0-4425-a580-0a69c359b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform['dryness_score'] = np.log1p(\n",
    "    np.clip(df_transform['dry_streak'] * df_transform['LST_Day_C'], 0, 20000)\n",
    ") / (df_transform['PRCP_7D_win'] + 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "156155a7-19c0-4e71-8250-a44bc5a0efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform['spread_score'] = np.log1p(\n",
    "    10 * df_transform['AWND_7D_win'] * df_transform['LST_Day_C'] * df_transform['is_dry']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "695dab4b-45bb-43fb-83bb-2bf04e34f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform['wind_temp_combo'] = np.clip((df_transform['AWND_7D_win'] * df_transform['TMAX_scaled']) ** 2, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53c679d1-14cd-4d9b-9792-29207051fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([\n",
    "    df_pca.reset_index(drop=True),\n",
    "    df_transform[[\n",
    "        'fire_risk_index', 'dryness_score', 'spread_score',\n",
    "        'wind_temp_combo'\n",
    "    ]].reset_index(drop=True)\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58b494d2-34aa-47d4-a80f-b65dba1718f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load df_pca and df_transform already processed externally\n",
    "# Assume df_pca and df_transform are available in memory\n",
    "\n",
    "# Combine PCA + engineered features for modeling\n",
    "df_model = pd.concat([\n",
    "    df_pca[['PC1', 'PC2', 'PC4', 'PC5', 'PC6', 'PC7', 'PC3']],\n",
    "    df_transform[['dryness_score', 'spread_score', 'Fire_Occurred', 'wind_temp_combo', 'fire_risk_index']]\n",
    "], axis=1)\n",
    "\n",
    "# Define features and target\n",
    "final_features = ['PC1', 'PC2', 'PC4', 'PC5', 'PC6', 'PC7', 'PC3',\n",
    "                  'dryness_score', 'spread_score', 'wind_temp_combo', 'fire_risk_index']\n",
    "X = df_model[final_features].copy()\n",
    "y = df_model['Fire_Occurred'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2b23f26-a1a0-4795-8f77-b1c74773601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 30k rows for faster experimentation\n",
    "X_sampled = X.sample(n=30000, random_state=42)\n",
    "y_sampled = y.loc[X_sampled.index]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sampled, y_sampled, test_size=0.2, stratify=y_sampled, random_state=42\n",
    ")\n",
    "\n",
    "# Loop over different k values for KNNImputer and test F1\n",
    "f1_results_sampled = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52c73b32-994f-43ff-8281-ed92abfb10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [2, 3, 5, 7, 10]:\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('imputer', KNNImputer(n_neighbors=k))\n",
    "    ])\n",
    "\n",
    "    # Process the data\n",
    "    X_train_processed = pipeline.fit_transform(X_train)\n",
    "    X_test_processed = pipeline.transform(X_test)\n",
    "\n",
    "    # Train SVM\n",
    "    svm_model = SVC(kernel='rbf', C=10, gamma='scale', class_weight={0: 1, 1: 5}, probability=True, random_state=42)\n",
    "    svm_model.fit(X_train_processed, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = svm_model.predict(X_test_processed)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "    f1_results_sampled.append((k, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13e81be5-87ea-48be-82c8-5931fa041c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "f1_df_sampled = pd.DataFrame(f1_results_sampled, columns=[\"k (neighbors)\", \"F1 Score (Fire)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "551ad284-7ef2-47dd-8424-68b51d2f5a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   k (neighbors)  F1 Score (Fire)\n",
      "0              2         0.047009\n",
      "1              3         0.042463\n",
      "2              5         0.042105\n",
      "3              7         0.041929\n",
      "4             10         0.042105\n"
     ]
    }
   ],
   "source": [
    "print(f1_df_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3e9ae7-d9af-415f-8027-5878f0c378ef",
   "metadata": {},
   "source": [
    "### highest F1 score gives optimum K to use in KNN for KNN Imputing before training svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95defcd8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
